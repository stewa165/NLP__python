# -*- coding: utf-8 -*-
"""Gensim Similarity (with language model LG).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SKOxIkX3PhDeXDYyOL9tbUlz-eWl1RNB

# Set Up Environement
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install -U spacy
!pip install newspaper3k
from newspaper import Article
import nltk
nltk.download('punkt')

!python -m spacy download en_core_web_lg
#!python -m spacy download en_core_web_trf

import spacy

from collections import defaultdict
from gensim import corpora

## little gensim
#pip install --upgrade gensim
### pip install --upgrade pyLDAvis
# %pip install pyLDAvis==2.1.2
import nltk
# python3 -m spacy download en
nltk.download('stopwords')
import re
import warnings
import numpy as np
import pandas as  pd
from pprint import pprint# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel# spaCy for preprocessing
import spacy# Plotting tools
import pyLDAvis
#import pyLDAvis.gensim_models
import pyLDAvis.gensim
#import pyLDAvis.gensim as gensimvis
import matplotlib.pyplot as plt
# %matplotlib inline
# NLTK Stop words
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])

"""# Creating the Corpus

### *Manually load in documents*
"""

documents = ["Vaccines are safe.", 
"Vaccines are fully effective.",
"People are considered fully protected.",
"People wonâ€™t get infected.",
"People are safe once they are vaccinated.",
"People are fully protected when they are vaccinated."
]

"""### *Scrape URL for documents*"""

url = 'https://www.cnbc.com/2021/06/28/covid-vaccines-work-but-more-people-need-to-get-the-shots-us-doctor.html'
article = Article(url)

article.download()
article.html
article.parse()

article.publish_date

article.text

article.nlp()
article.keywords #14

# Setup Language Model
lang_model = spacy.load("en_core_web_lg")
#lang_model = spacy.load("en_core_web_trf")
# Merge noun phrases and entities for easier analysis
lang_model.add_pipe('merge_entities')
#lang_model.add_pipe('merge_noun_chunks')
#lang_model.add_pipe(lang_model.create_pipe('merge_entities'))
#lang_model.add_pipe(lang_model.create_pipe('merge_noun_chunks'))
lang_model.add_pipe('sentencizer')
# Article Data Structures
article_text = []
article_paragraphs = []
article_sentences = []
artilce_bow = []
artilce_bow_nlp = []

article_sentences = []
documents = str(article.text)
article_lang_model = lang_model(documents)
for sent in article_lang_model.sents:
  article_sentences.append(sent)

for s in article_sentences:
  print(s)

data = article_sentences
def Convert(string): 
    li = list(string.split(" ")) 
    return li

#Convert(data)    
#print(data)
def sent_to_words(sentences):
  for sentence in article_sentences:
     yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))            #deacc=True removes punctuations
#    yield(gensim.utils.simple_preprocess(data, deacc=False, min_len=2, max_len=15))
data_words = list(sent_to_words(data))
  
print(*data_words)

# Build the bigram and trigram models
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)
# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)
# See trigram example
#print(trigram_mod[bigram_mod[data_words[0]]])

# Define function for stopwords, bigrams, trigrams and lemmatization
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
 
def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]
 
def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]
 
def lemmatization(texts):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = lang_model(" ".join(sent))
        texts_out.append([token.lemma_ for token in doc])
    return texts_out

# Remove Stop Words
data_words_nostops = remove_stopwords(data_words)
#print(data_words_nostops)
 
# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)
##print(data_words_bigrams)
 
# Form Trigrams
#data_words_trigrams = make_trigrams(data_words_nostops)
##print(data_words_bigrams)

# Do lemmatization keeping only noun, adj, vb, adv
data_lemmatized = lemmatization(data_words_bigrams)
 
#print(data_lemmatized[:1])

# Create Dictionary
##id2word = corpora.Dictionary(data_words_bigrams)
id2word = corpora.Dictionary(data_lemmatized)

# Create Corpus
#article_bow = data_words_nostops
##article_bow = data_words_bigrams
#article_bow = data_words_trigrams
article_bow = data_lemmatized
print(article_bow)

"""*Remove words that only appear once*"""

# remove words that appear only once
frequency = defaultdict(int)
for text in article_bow:
    for token in text:
        frequency[token] += 1

texts = [
    [token for token in text if frequency[token] > 1]
    for text in article_bow
]

dictionary = id2word
corpus = [dictionary.doc2bow(text) for text in article_bow]
print(dictionary)
print(corpus)

"""# Similarity Interface

# LDA
"""

#NOTE - If your topics still do not make sense, try increasing passes and iterations, while increasing chunksize to the extent your memory can handle.

topic_count = 7
# Term Document Frequency
#corpus = [id2word.doc2bow(article_text) for article_text in article_bow]
#corpus = [id2word.doc2bow(['company','filed','trial','discussed','abuses','reigns','lowered', 'stopped', 'odds'], allow_update=True)]
print("id2word: ", dictionary)
print("Corpus: ", corpus)
print("article_bow: ", article_bow)
#for x in id2word:
#  print("x: ", id2word[x])
#print("Corpus ",corpus[:1])
#[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]
#print("id2word: ", id2word)
warnings.filterwarnings("ignore")
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=dictionary,
                                           num_topics=topic_count,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=10000,
                                           passes=10,
                                           iterations=100,
                                           alpha='auto',
                                           per_word_topics=True)
#ts = lda_model.get_document_topics(corpus)
#ts = lda_model.get_topics()
topics = []
for i in range(topic_count):
  words = []
  ts = lda_model.get_topic_terms(i)
  for t in ts:
    words.append([i, t, id2word.get(t[0])])
  topics.append(words)

print(topics)
for t in topics:
  for x in t:
    print(x[0], x[1], x[2])

doc_lda = lda_model[corpus]
# Compute Perplexity
print('\nPerplexity: ', lda_model.log_perplexity(corpus))
# a measure of how good the model is. lower the better.
 
# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=article_bow, dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

warnings.filterwarnings("ignore")
# Print the keyword of topics
topics = lda_model.get_document_topics(corpus)
print("TOPICS:")
#for topic in topics:
#  print(topic)
#print("TOPICS: ", topics)
#print("TOPICS: ", doc_lda)
#print("TOPICS: ", lda_model.print_topics())

# Visualize the topics
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, mds='mmds')

vis

# NOTE - if the same words start to appear across multiple topics, the number of topics is too high.

# supporting function

#If your topics still do not make sense, try increasing passes and iterations, while increasing chunksize to the extent your memory can handle.

def compute_coherence_values(corpus, dictionary, k, a, b):
    
    lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                           id2word=dictionary,
                                           num_topics=k, 
                                           random_state=100,
                                           chunksize=10000,
                                           passes=10, #increasing passes increases runtime
                                           iterations=100,
                                           alpha=a,
                                           eta=b)
    
    coherence_model_lda = CoherenceModel(model=lda_model, texts=article_bow, dictionary=dictionary, coherence='c_v')
    
    return coherence_model_lda.get_coherence()

import numpy as np
import tqdm
grid = {}
grid['Validation_Set'] = {}
# Topics range
min_topics = 2
max_topics = 20
step_size = 1
topics_range = range(min_topics, max_topics, step_size)
# Alpha parameter
alpha = list(np.arange(0.01, 1, 0.3))
alpha.append('symmetric')
alpha.append('asymmetric')
# Beta parameter
beta = list(np.arange(0.01, 1, 0.3))
beta.append('symmetric')
# Validation sets
num_of_docs = len(corpus)
corpus_sets = [# gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.25)), 
               # gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.5)), 
                gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), 
               corpus]
corpus_title = ['75% Corpus', '100% Corpus']
model_results = {'Validation_Set': [],
                 'Topics': [],
                 'Alpha': [],
                 'Beta': [],
                 'Coherence': []
                }
# Can take a long time to run
if 1 == 1:
    pbar = tqdm.tqdm(total=540)
    
    # iterate through validation corpuses
    for i in range(len(corpus_sets)):
        # iterate through number of topics
        for k in topics_range:
            # iterate through alpha values
            for a in alpha:
                # iterare through beta values
                for b in beta:
                    # get the coherence score for the given parameters
                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary, 
                                                  k=k, a=a, b=b)
                    # Save the model results
                    model_results['Validation_Set'].append(corpus_title[i])
                    model_results['Topics'].append(k)
                    model_results['Alpha'].append(a)
                    model_results['Beta'].append(b)
                    model_results['Coherence'].append(cv)
                    
                    pbar.update(1)
    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)
    pbar.close()

import logging
logging.basicConfig(filename='gensim.log',
                    format="%(asctime)s:%(levelname)s:%(message)s",
                    level=logging.INFO)

import pandas as pd
# filter the results
results = pd.read_csv('lda_tuning_results.csv')
results_for_vis = results[results['Alpha']=='0.01']
results_for_vis = results_for_vis[results_for_vis['Beta']=='0.01']
# now the plot
import matplotlib.pyplot as plt
plt.plot(np.array(results_for_vis['Topics']), np.array(results_for_vis['Coherence']))
plt.scatter(np.array(results_for_vis['Topics']), np.array(results_for_vis['Coherence']))
plt.ylabel('C_v Score')
plt.xlabel('Number of Topics')
plt.xticks(np.array(results_for_vis['Topics']))
plt.show()

print(results)

#import pandas as pd
#col_list = ["Validation_Set", "Topics", "Alpha", "Beta", "Coherence"]
#df = pd.read_csv("lda_tuning_results.csv", usecols=col_list)
#print(df)

#import matplotlib.pyplot as plt
#plt.plot(df.Topics, df.Coherence)
#plt.xlabel('Number of Topics')
#plt.ylabel('Coherence Score')
#plt.show()